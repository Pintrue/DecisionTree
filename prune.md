## Prune
The main logic of prune is implemented in function prune\_help, which is a helper function for head-recursion depth first search. There are 3 arguments for this function: the current node that is being tested for pruning, the root node that remains same for all recursion, and the validation data as the data for pruning. The first arguments are used for recursion and the second and third arguments are used to test the tree. The main idea is to test all nodes whose 2 children are both leaves, and try to prune it using the results from both sides. This produce 3 cases: unmodified tree, pruned tree using label from left child node, and pruned tree using label from right child node. Then use the root node, which is copied by reference into the function, to evaluate the tree using the validation data for all 3 cases, compare the their performances, and take the decision with minimum mistakes.
The reason why we use head recursion is that we want to prune the subtree before testing the node, because the node that do not have 2 leaves as children may have 2 leaves after pruning its children.
The influence to the clean dataset is not obvious: when testing using test sets, the overall accuracy remains almost unchanged, with some test sets better and some test sets worse only in small amount. This is because we prune the tree using the validation set and test it using the test set, so it is possible for the result to be a little bit worse for some test sets. However, when testing using noisy dataset, the accuracy increases almost for every test set. I think the primary reason is that the noisy data is more likely to produce over-fitting so pruning almost always makes the results better.
